{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Memory Transformer-XL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO+7eXFLGQNyEfPuI0KJDMK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/napoler/mlmxl-pytorch/blob/master/Memory_Transformer_XL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L40CAu3TIsAD",
        "outputId": "cbc5ea0c-98c3-4e6a-f7db-52377c7a9dfa"
      },
      "source": [
        "!Memory Transformer-XL"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: Memory: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fl_YYGWKyj4",
        "outputId": "01b1bbed-118b-427b-9c0f-3ddbfb51ef5f"
      },
      "source": [
        "!pip install memory-transformer-xl\n",
        "!pip install transformers\n",
        "!pip install mlm-pytorch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: memory-transformer-xl in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: mogrifier in /usr/local/lib/python3.7/dist-packages (from memory-transformer-xl) (0.0.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from memory-transformer-xl) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->memory-transformer-xl) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->memory-transformer-xl) (3.7.4.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: mlm-pytorch in /usr/local/lib/python3.7/dist-packages (0.0.3)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mlm-pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->mlm-pytorch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->mlm-pytorch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zyv36DoK75H"
      },
      "source": [
        "import torch\n",
        "from memory_transformer_xl import MemoryTransformerXL\n",
        "\n",
        "model = MemoryTransformerXL(\n",
        "    num_tokens = 20000,\n",
        "    dim = 1024,\n",
        "    heads = 8,\n",
        "    depth = 8,\n",
        "    seq_len = 512,\n",
        "    mem_len = 256,            # short term memory (the memory from transformer-xl)\n",
        "    lmem_len = 256,           # long term memory (memory attention network attending to short term memory and hidden activations)\n",
        "    mem_write_iters = 2,      # number of iterations of attention for writing to memory\n",
        "    memory_layers = [6,7,8],  # which layers to use memory, only the later layers are actually needed\n",
        "    num_mem_kv = 128,         # number of memory key/values, from All-attention paper\n",
        "\n",
        ").cuda()\n",
        "\n",
        "x1 = torch.randint(0, 20000, (1, 512)).cuda()\n",
        "logits1, mem1 = model(x1)\n",
        "\n",
        "x2 = torch.randint(0, 20000, (1, 512)).cuda()\n",
        "logits2, mem2 = model(x2, memories = mem1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJlHpMRELN0F",
        "outputId": "20451ae7-b6c1-4595-e2b9-921cdb6baf9d"
      },
      "source": [
        "mem2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Memory(short=tensor([[[[ 0.0362,  0.5993,  0.2789,  ..., -0.0101,  0.3508, -0.3563],\n",
              "          [-0.0494,  0.2674,  0.4897,  ..., -0.4644, -0.7191, -0.4886],\n",
              "          [-0.3707,  0.2959,  0.2633,  ...,  0.2003, -0.4072, -0.5428],\n",
              "          ...,\n",
              "          [ 0.3973,  0.6157,  0.5027,  ...,  0.5756, -0.5070, -0.6027],\n",
              "          [ 0.1458, -0.1717, -0.2221,  ..., -0.1036, -0.4442, -0.0377],\n",
              "          [-0.3231, -0.1347,  0.6932,  ..., -0.3780,  0.1821, -0.5134]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0937,  0.5486,  0.1062,  ...,  0.0058,  0.3140, -0.3302],\n",
              "          [-0.0363,  0.2995,  0.4309,  ..., -0.3821, -0.5070, -0.4848],\n",
              "          [-0.4930,  0.2653,  0.2242,  ...,  0.1923, -0.4976, -0.4233],\n",
              "          ...,\n",
              "          [ 0.3066,  0.5526,  0.5867,  ...,  0.5526, -0.5247, -0.5976],\n",
              "          [ 0.2169, -0.1093, -0.2304,  ..., -0.0869, -0.4577, -0.0665],\n",
              "          [-0.1896, -0.1146,  0.6905,  ..., -0.3956,  0.1697, -0.5003]]],\n",
              "\n",
              "\n",
              "        [[[ 0.0673,  0.5409,  0.1710,  ...,  0.0041,  0.3060, -0.3003],\n",
              "          [-0.1263,  0.2941,  0.4729,  ..., -0.3363, -0.3828, -0.4906],\n",
              "          [-0.4580,  0.2913,  0.3545,  ...,  0.2308, -0.5208, -0.4890],\n",
              "          ...,\n",
              "          [ 0.2292,  0.4814,  0.5797,  ...,  0.5944, -0.3788, -0.6256],\n",
              "          [ 0.3051, -0.0280, -0.1173,  ..., -0.0789, -0.4150, -0.1959],\n",
              "          [-0.2353, -0.1030,  0.6809,  ..., -0.4614,  0.1271, -0.3898]]]],\n",
              "       device='cuda:0'), long=tensor([[[-0.0111, -0.0744,  0.1080,  ..., -0.0353,  0.0939,  0.0130],\n",
              "         [-0.0101,  0.0045, -0.0192,  ..., -0.0984,  0.0214,  0.0190],\n",
              "         [-0.0778,  0.0129,  0.0146,  ..., -0.0231,  0.0385,  0.0273],\n",
              "         ...,\n",
              "         [ 0.0464, -0.1024,  0.0916,  ...,  0.0112,  0.0729, -0.0159],\n",
              "         [ 0.0520, -0.0277,  0.1076,  ..., -0.0675,  0.0640, -0.0088],\n",
              "         [ 0.0048, -0.0397, -0.0002,  ..., -0.0234,  0.0871,  0.0806]]],\n",
              "       device='cuda:0', grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5OxkLqPMhb2"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIgrTMHQMhx8"
      },
      "source": [
        "# 分词\n",
        "数据特点：\n",
        "可直接用于预训练、语言模型或语言生成任务。\n",
        "发布专用于简体中文NLP任务的小词表。\n",
        "词表介绍\n",
        "Google原始中文词表和我们发布的小词表的统计信息如下：\n",
        "\n",
        "Token Type\tGoogle\tCLUE\n",
        "Simplified Chinese\t11378\t5689\n",
        "Traditional Chinese\t3264\t✗\n",
        "English\t3529\t1320\n",
        "Japanese\t573\t✗\n",
        "Korean\t84\t✗\n",
        "Emoji\t56\t✗\n",
        "Numbers\t1179\t140\n",
        "Special Tokens\t106\t106\n",
        "Other Tokens\t959\t766\n",
        "Total\t21128\t8021\n",
        "https://github.com/CLUEbenchmark/CLUEPretrainedModels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPNjiqH2Mq6L"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel,BertTokenizer\n",
        "  \n",
        "tokenizer = BertTokenizer.from_pretrained(\"clue/roberta_chinese_clue_tiny\")\n",
        "\n",
        "# model = AutoModel.from_pretrained(\"clue/roberta_chinese_clue_tiny\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl3QCP-GM9bt",
        "outputId": "5cd44d38-7204-47a2-b7af-77986d845b66"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreTrainedTokenizer(name_or_path='clue/roberta_chinese_clue_tiny', vocab_size=8021, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MybAvPCP_uT"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hc4wmOwLLcZ",
        "outputId": "d71265f9-48c5-4558-d56c-25ecf21aaa27"
      },
      "source": [
        "tokenizer.vocab_size"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8021"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evp0oyz5QAKg",
        "outputId": "84c395df-c7a8-43f1-f209-2c7743cf3e5a"
      },
      "source": [
        "dir(tokenizer)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SPECIAL_TOKENS_ATTRIBUTES',\n",
              " '__annotations__',\n",
              " '__call__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_add_tokens',\n",
              " '_additional_special_tokens',\n",
              " '_batch_encode_plus',\n",
              " '_batch_prepare_for_model',\n",
              " '_bos_token',\n",
              " '_cls_token',\n",
              " '_convert_id_to_token',\n",
              " '_convert_token_to_id',\n",
              " '_convert_token_to_id_with_added_voc',\n",
              " '_decode',\n",
              " '_decode_use_source_tokenizer',\n",
              " '_encode_plus',\n",
              " '_eos_token',\n",
              " '_eventual_warn_about_too_long_sequence',\n",
              " '_from_pretrained',\n",
              " '_get_padding_truncation_strategies',\n",
              " '_mask_token',\n",
              " '_pad',\n",
              " '_pad_token',\n",
              " '_pad_token_type_id',\n",
              " '_push_to_hub',\n",
              " '_save_pretrained',\n",
              " '_sep_token',\n",
              " '_tokenize',\n",
              " '_unk_token',\n",
              " 'add_special_tokens',\n",
              " 'add_tokens',\n",
              " 'added_tokens_decoder',\n",
              " 'added_tokens_encoder',\n",
              " 'additional_special_tokens',\n",
              " 'additional_special_tokens_ids',\n",
              " 'all_special_ids',\n",
              " 'all_special_tokens',\n",
              " 'all_special_tokens_extended',\n",
              " 'as_target_tokenizer',\n",
              " 'basic_tokenizer',\n",
              " 'batch_decode',\n",
              " 'batch_encode_plus',\n",
              " 'bos_token',\n",
              " 'bos_token_id',\n",
              " 'build_inputs_with_special_tokens',\n",
              " 'clean_up_tokenization',\n",
              " 'cls_token',\n",
              " 'cls_token_id',\n",
              " 'convert_ids_to_tokens',\n",
              " 'convert_tokens_to_ids',\n",
              " 'convert_tokens_to_string',\n",
              " 'create_token_type_ids_from_sequences',\n",
              " 'decode',\n",
              " 'deprecation_warnings',\n",
              " 'do_basic_tokenize',\n",
              " 'do_lower_case',\n",
              " 'encode',\n",
              " 'encode_plus',\n",
              " 'eos_token',\n",
              " 'eos_token_id',\n",
              " 'from_pretrained',\n",
              " 'get_added_vocab',\n",
              " 'get_special_tokens_mask',\n",
              " 'get_vocab',\n",
              " 'ids_to_tokens',\n",
              " 'init_inputs',\n",
              " 'init_kwargs',\n",
              " 'is_fast',\n",
              " 'mask_token',\n",
              " 'mask_token_id',\n",
              " 'max_len_sentences_pair',\n",
              " 'max_len_single_sentence',\n",
              " 'max_model_input_sizes',\n",
              " 'model_input_names',\n",
              " 'model_max_length',\n",
              " 'name_or_path',\n",
              " 'num_special_tokens_to_add',\n",
              " 'pad',\n",
              " 'pad_token',\n",
              " 'pad_token_id',\n",
              " 'pad_token_type_id',\n",
              " 'padding_side',\n",
              " 'prepare_for_model',\n",
              " 'prepare_for_tokenization',\n",
              " 'prepare_seq2seq_batch',\n",
              " 'pretrained_init_configuration',\n",
              " 'pretrained_vocab_files_map',\n",
              " 'push_to_hub',\n",
              " 'sanitize_special_tokens',\n",
              " 'save_pretrained',\n",
              " 'save_vocabulary',\n",
              " 'sep_token',\n",
              " 'sep_token_id',\n",
              " 'slow_tokenizer_class',\n",
              " 'special_tokens_map',\n",
              " 'special_tokens_map_extended',\n",
              " 'tokenize',\n",
              " 'truncate_sequences',\n",
              " 'unique_no_split_tokens',\n",
              " 'unk_token',\n",
              " 'unk_token_id',\n",
              " 'verbose',\n",
              " 'vocab',\n",
              " 'vocab_files_names',\n",
              " 'vocab_size',\n",
              " 'wordpiece_tokenizer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuaZnZ2wNQKj"
      },
      "source": [
        "# 模型测试\n",
        "\n",
        "使用mlm模式训练\n",
        "https://github.com/lucidrains/mlm-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6-iOR52NUNt"
      },
      "source": [
        "# vocab_size"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbB84zr1Rb53"
      },
      "source": [
        "## 重新mlm模型\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xWjqtquRgi6"
      },
      "source": [
        "import math\n",
        "from functools import reduce\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# helpers\n",
        "\n",
        "def prob_mask_like(t, prob):\n",
        "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
        "\n",
        "def mask_with_tokens(t, token_ids):\n",
        "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
        "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
        "    return mask\n",
        "\n",
        "def get_mask_subset_with_prob(mask, prob):\n",
        "    batch, seq_len, device = *mask.shape, mask.device\n",
        "    max_masked = math.ceil(prob * seq_len)\n",
        "\n",
        "    num_tokens = mask.sum(dim=-1, keepdim=True)\n",
        "    mask_excess = (mask.cumsum(dim=-1) > (num_tokens * prob).ceil())\n",
        "    mask_excess = mask_excess[:, :max_masked]\n",
        "\n",
        "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n",
        "    _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
        "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)\n",
        "\n",
        "    new_mask = torch.zeros((batch, seq_len + 1), device=device)\n",
        "    new_mask.scatter_(-1, sampled_indices, 1)\n",
        "    return new_mask[:, 1:].bool()\n",
        "\n",
        "# main class\n",
        "\n",
        "class MLMXL(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        transformer,\n",
        "        mask_prob = 0.15,\n",
        "        replace_prob = 0.9,\n",
        "        num_tokens = None,\n",
        "        random_token_prob = 0.,\n",
        "        mask_token_id = 2,\n",
        "        pad_token_id = 0,\n",
        "        mask_ignore_token_ids = []):\n",
        "        super().__init__()\n",
        "\n",
        "        self.transformer = transformer\n",
        "        self.mem=None\n",
        "\n",
        "        # mlm related probabilities\n",
        "        self.mask_prob = mask_prob\n",
        "        self.replace_prob = replace_prob\n",
        "\n",
        "        self.num_tokens = num_tokens\n",
        "        self.random_token_prob = random_token_prob\n",
        "\n",
        "        # token ids\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.mask_token_id = mask_token_id\n",
        "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
        "\n",
        "    def forward(self, input, **kwargs):\n",
        "        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
        "        # also do not include these special tokens in the tokens chosen at random\n",
        "        no_mask = mask_with_tokens(input, self.mask_ignore_token_ids)\n",
        "        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n",
        "\n",
        "        # get mask indices\n",
        "        mask_indices = torch.nonzero(mask, as_tuple=True)\n",
        "\n",
        "        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
        "        masked_input = input.clone().detach()\n",
        "\n",
        "        # if random token probability > 0 for mlm\n",
        "        if self.random_token_prob > 0:\n",
        "            assert self.num_tokens is not None, 'num_tokens keyword must be supplied when instantiating MLM if using random token replacement'\n",
        "            random_token_prob = prob_mask_like(input, self.random_token_prob)\n",
        "            random_tokens = torch.randint(0, self.num_tokens, input.shape, device=input.device)\n",
        "            random_no_mask = mask_with_tokens(random_tokens, self.mask_ignore_token_ids)\n",
        "            random_token_prob &= ~random_no_mask\n",
        "            random_indices = torch.nonzero(random_token_prob, as_tuple=True)\n",
        "            masked_input[random_indices] = random_tokens[random_indices]\n",
        "\n",
        "        # [mask] input\n",
        "        replace_prob = prob_mask_like(input, self.replace_prob)\n",
        "        masked_input = masked_input.masked_fill(mask * replace_prob, self.mask_token_id)\n",
        "\n",
        "        # mask out any tokens to padding tokens that were not originally going to be masked\n",
        "        labels = input.masked_fill(~mask, self.pad_token_id)\n",
        "        if self.mem!=None:\n",
        "        # get generator output and get mlm loss\n",
        "          logits,self.mem = self.transformer(masked_input, memories = self.mem, **kwargs)\n",
        "        else:\n",
        "          logits,self.mem = self.transformer(masked_input, **kwargs)\n",
        "\n",
        "        mlm_loss = F.cross_entropy(\n",
        "            logits.transpose(1, 2),\n",
        "            labels,\n",
        "            ignore_index = self.pad_token_id\n",
        "        )\n",
        "\n",
        "        return mlm_loss\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-IU2T2UNOw5"
      },
      "source": [
        "import torch\n",
        "from memory_transformer_xl import MemoryTransformerXL\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "# from mlm_pytorch import MLM\n",
        "\n",
        "model = MemoryTransformerXL(\n",
        "    num_tokens = tokenizer.vocab_size,\n",
        "    dim = 128,\n",
        "    heads = 8,\n",
        "    depth = 8,\n",
        "    seq_len = 1024,\n",
        "    mem_len = 256,            # short term memory (the memory from transformer-xl)\n",
        "    lmem_len = 256,           # long term memory (memory attention network attending to short term memory and hidden activations)\n",
        "    mem_write_iters = 2,      # number of iterations of attention for writing to memory\n",
        "    memory_layers = [6,7,8],  # which layers to use memory, only the later layers are actually needed\n",
        "    num_mem_kv = 128,         # number of memory key/values, from All-attention paper\n",
        "\n",
        ").cuda()\n",
        "\n",
        "x1 = torch.randint(0, tokenizer.vocab_size, (1, 1024)).cuda()\n",
        "logits1, mem1 = model(x1)\n",
        "\n",
        "x2 = torch.randint(0, tokenizer.vocab_size, (1, 1024)).cuda()\n",
        "logits2, mem2 = model(x2, memories = mem1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5dm_ttEP4cZ"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h83e2MA6OMLq"
      },
      "source": [
        "torch.save(model.state_dict(), \"model1024.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPHPr8PkP5xB"
      },
      "source": [
        "# plugin the language model into the MLM trainer\n",
        "\n",
        "trainer = MLMXL(\n",
        "    model,\n",
        "    mask_token_id = tokenizer.mask_token_id,          # the token id reserved for masking\n",
        "    pad_token_id = tokenizer.pad_token_id,           # the token id for padding\n",
        "    mask_prob = 0.15,           # masking probability for masked language modeling\n",
        "    replace_prob = 0.90,        # ~10% probability that token will not be masked, but included in loss, as detailed in the epaper\n",
        "    mask_ignore_token_ids = [tokenizer.cls_token_id,tokenizer.sep_token_id]  # other tokens to exclude from masking, include the [cls] and [sep] here\n",
        ").cuda()\n",
        "\n",
        "# optimizer\n",
        "\n",
        "opt = Adam(trainer.parameters(), lr=3e-4)\n",
        "\n",
        "# one training step (do this for many steps in a for loop, getting new `data` each time)\n",
        "\n",
        "data = torch.randint(0, tokenizer.vocab_size, (2, 1024)).cuda()\n",
        "\n",
        "loss = trainer(data)\n",
        "loss.backward()\n",
        "opt.step()\n",
        "opt.zero_grad()\n",
        "\n",
        "# after much training, the model should have improved for downstream tasks\n",
        "\n",
        "# torch.save(transformer, f'./pretrained-model.pt')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMCmW8DtSLP-",
        "outputId": "0981213a-c488-47f6-e89d-5e26eb77b70e"
      },
      "source": [
        "loss"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9.0243, device='cuda:0', grad_fn=<NllLoss2DBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "log9_CFzN7Qn"
      },
      "source": [
        "# dir(model)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WVOOqD8NkvO"
      },
      "source": [
        "logits2"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}